{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Question 1: What is a Support Vector Machine (SVM), and how does it work?\n",
        "Answer:\n",
        "\n",
        "A Support Vector Machine (SVM) is a supervised machine learning algorithm used for classification and regression tasks, but it is mostly used for binary classification.\n",
        "\n",
        "‚úÖ How SVM Works:\n",
        "\n",
        "1.Goal:\n",
        "SVM tries to find the best separating hyperplane that divides the data into classes with the maximum margin (i.e., the greatest possible distance between the hyperplane and the nearest data points of each class).\n",
        "\n",
        "2.Support Vectors:\n",
        "These are the data points closest to the hyperplane. They are critical in defining the decision boundary. The model uses these points to maximize the margin.\n",
        "\n",
        "3.Margin:\n",
        "The margin is the distance between the hyperplane and the support vectors. SVM aims to maximize this margin to improve generalization on unseen data.\n",
        "\n",
        "4.Linear vs. Non-linear Data:\n",
        "\n",
        "For linearly separable data, SVM finds a straight line (in 2D) or hyperplane (in higher dimensions).\n",
        "For non-linear data, SVM uses the kernel trick to map data into a higher-dimensional space where a linear separator can be found.\n"
      ],
      "metadata": {
        "id": "91AzRWnu6UWK"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QIsZZvLD6uiQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 2: Explain the difference between Hard Margin and Soft Margin SVM.\n",
        "\n",
        "Answer:\n",
        "The difference between Hard Margin and Soft Margin SVM lies in how strictly the model separates the classes, especially when the data is not perfectly separable.\n",
        "\n",
        " 1. Hard Margin SVM\n",
        "Definition:\n",
        "Hard Margin SVM tries to find a hyperplane that perfectly separates the classes without any misclassification.\n",
        "\n",
        "Assumption:\n",
        "It assumes that the data is linearly separable.\n",
        "\n",
        "Characteristics:\n",
        "\n",
        "No tolerance for misclassified points.\n",
        "\n",
        "Maximizes the margin strictly.\n",
        "\n",
        "Fails if data is noisy or overlaps.\n",
        "\n",
        "When to Use:\n",
        "Only when the dataset is clean and perfectly separable.\n",
        "\n",
        " 2. Soft Margin SVM\n",
        "Definition:\n",
        "Soft Margin SVM allows some misclassification of data points to improve the model‚Äôs ability to generalize to unseen data.\n",
        "\n",
        "Assumption:\n",
        "Works well even if the data is not linearly separable.\n",
        "\n",
        "Characteristics:\n",
        "\n",
        "Introduces slack variables to allow violations of the margin.\n",
        "\n",
        "Adds a regularization parameter (C) to control the trade-off between maximizing the margin and minimizing classification errors:\n",
        "\n",
        "High C ‚Üí less tolerance for misclassification.\n",
        "\n",
        "Low C ‚Üí more tolerance, larger margin.\n",
        "\n",
        "| Feature             | Hard Margin SVM          | Soft Margin SVM                  |\n",
        "| ------------------- | ------------------------ | -------------------------------- |\n",
        "| Tolerance to errors | No (strict separation)   | Yes (allows misclassification)   |\n",
        "| Data requirement    | Perfectly separable data | Noisy or overlapping data        |\n",
        "| Flexibility         | Less flexible            | More flexible                    |\n",
        "| Regularization (C)  | Not used                 | Used to balance margin and error |\n",
        "\n"
      ],
      "metadata": {
        "id": "vHN8wkzo667T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 3: What is the Kernel Trick in SVM? Give one example of a kernel and\n",
        "explain its use case.\n",
        "Answer:\n",
        "\n",
        "he Kernel Trick is a mathematical technique used in SVM to handle non-linearly separable data by implicitly mapping it into a higher-dimensional space, without actually computing the coordinates in that space.\n",
        "\n",
        "This allows SVM to find a linear hyperplane in the transformed space that corresponds to a non-linear boundary in the original input space.\n",
        "\n",
        "‚úÖ Why Use the Kernel Trick?\n",
        "Real-world data is often not linearly separable in its original form.\n",
        "\n",
        "Mapping to a higher dimension makes it possible to apply linear classification in a non-linear problem.\n",
        "\n",
        "The Kernel Trick avoids the computational cost of explicitly transforming the data."
      ],
      "metadata": {
        "id": "vo-Ou4Ye7VnW"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kji-jCTQ7KaM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 4: What is a Na√Øve Bayes Classifier, and why is it called ‚Äúna√Øve‚Äù?\n",
        "\n",
        "Answer:\n",
        "\n",
        "The Na√Øve Bayes classifier is a supervised learning algorithm based on Bayes‚Äô Theorem, primarily used for classification tasks. It calculates the probability that a given instance belongs to a particular class based on its features.\n",
        "\n",
        "It is called \"na√Øve\" because it assumes that all features are independent of each other given the class label.\n",
        "\n",
        "üîπ In real-world data, this independence assumption is often false, but the model still performs well in many applications ‚Äî hence it's na√Øvely simple, yet effective."
      ],
      "metadata": {
        "id": "fHx59Ggw7nfr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 5: Describe the Gaussian, Multinomial, and Bernoulli Na√Øve Bayes variants.\n",
        "When would you use each one?\n",
        "\n",
        "Answer:\n",
        "\n",
        "1. Gaussian Na√Øve Bayes\n",
        "Assumption:\n",
        "Features follow a normal (Gaussian) distribution.\n",
        "\n",
        "Use Case:\n",
        "When the input features are continuous (real-valued) ‚Äî such as height, weight, temperature, etc.\n",
        "\n",
        "How it works:\n",
        "For each feature, it calculates the probability using the Gaussian probability density function.\n",
        "\n",
        "Example Applications:\n",
        "\n",
        "Medical diagnosis (e.g., predicting diseases from lab test values)\n",
        "\n",
        "Sensor data analysis\n",
        "\n",
        "Any classification task with numeric features\n",
        "\n",
        " 2. Multinomial Na√Øve Bayes\n",
        "Assumption:\n",
        "Features represent discrete counts (e.g., frequency of words).\n",
        "\n",
        "Use Case:\n",
        "Best suited for document classification and natural language processing (NLP), where input features are word counts or term frequencies.\n",
        "\n",
        "How it works:\n",
        "Calculates the probability of features (word counts) occurring in a class using multinomial distribution.\n",
        "\n",
        "Example Applications:\n",
        "\n",
        "Spam filtering\n",
        "\n",
        "News article classification\n",
        "\n",
        "Sentiment analysis (bag-of-words model)\n",
        "\n",
        " 3. Bernoulli Na√Øve Bayes\n",
        "Assumption:\n",
        "Features are binary (0 or 1) ‚Äî indicating the presence or absence of a feature.\n",
        "\n",
        "Use Case:\n",
        "Also used in text classification, especially when you care about whether a word occurs, not how many times.\n",
        "\n",
        "How it works:\n",
        "Uses the Bernoulli distribution to estimate probabilities based on binary features.\n",
        "\n",
        "Example Applications:\n",
        "\n",
        "Email spam detection\n",
        "\n",
        "Text classification with binary word indicators\n",
        "\n",
        "Feature presence/absence modeling\n",
        "\n"
      ],
      "metadata": {
        "id": "_Skvlh_l8ACn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "‚óè You can use any suitable datasets like Iris, Breast Cancer, or Wine from\n",
        "sklearn.datasets or a CSV file you have.\n",
        "Question 6: Write a Python program to:\n",
        "‚óè Load the Iris dataset\n",
        "‚óè Train an SVM Classifier with a linear kernel\n",
        "‚óè Print the model's accuracy and support vectors."
      ],
      "metadata": {
        "id": "7Yx2skDR8xXY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# 1. Load the Iris dataset\n",
        "iris = datasets.load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# 2. Split into train and test sets (80-20 split)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# 3. Train an SVM classifier with linear kernel\n",
        "model = SVC(kernel='linear')\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# 4. Predict and evaluate\n",
        "y_pred = model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# 5. Print results\n",
        "print(\"Model Accuracy:\", accuracy)\n",
        "print(\"Support Vectors:\\n\", model.support_vectors_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3NF1tgDa83VW",
        "outputId": "2f3d7b6e-5d8c-46ec-d1cd-19cc700b8a1a"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 1.0\n",
            "Support Vectors:\n",
            " [[4.8 3.4 1.9 0.2]\n",
            " [5.1 3.3 1.7 0.5]\n",
            " [4.5 2.3 1.3 0.3]\n",
            " [5.6 3.  4.5 1.5]\n",
            " [5.4 3.  4.5 1.5]\n",
            " [6.7 3.  5.  1.7]\n",
            " [5.9 3.2 4.8 1.8]\n",
            " [5.1 2.5 3.  1.1]\n",
            " [6.  2.7 5.1 1.6]\n",
            " [6.3 2.5 4.9 1.5]\n",
            " [6.1 2.9 4.7 1.4]\n",
            " [6.5 2.8 4.6 1.5]\n",
            " [6.9 3.1 4.9 1.5]\n",
            " [6.3 2.3 4.4 1.3]\n",
            " [6.3 2.5 5.  1.9]\n",
            " [6.3 2.8 5.1 1.5]\n",
            " [6.3 2.7 4.9 1.8]\n",
            " [6.  3.  4.8 1.8]\n",
            " [6.  2.2 5.  1.5]\n",
            " [6.2 2.8 4.8 1.8]\n",
            " [6.5 3.  5.2 2. ]\n",
            " [7.2 3.  5.8 1.6]\n",
            " [5.6 2.8 4.9 2. ]\n",
            " [5.9 3.  5.1 1.8]\n",
            " [4.9 2.5 4.5 1.7]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 7: Write a Python program to:\n",
        "‚óè Load the Breast Cancer dataset\n",
        "‚óè Train a Gaussian Na√Øve Bayes model\n",
        "‚óè Print its classification report including precision, recall, and F1-score."
      ],
      "metadata": {
        "id": "XOXleIm79LgD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# 1. Load the Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# 2. Split into training and testing sets (80-20 split)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# 3. Train Gaussian Na√Øve Bayes model\n",
        "model = GaussianNB()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# 4. Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# 5. Print classification report\n",
        "print(\"Classification Report:\\n\")\n",
        "print(classification_report(y_test, y_pred, target_names=data.target_names))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rvXqElZh9NUk",
        "outputId": "e84accc7-5954-47aa-bb77-66d7bd8c481d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   malignant       1.00      0.93      0.96        43\n",
            "      benign       0.96      1.00      0.98        71\n",
            "\n",
            "    accuracy                           0.97       114\n",
            "   macro avg       0.98      0.97      0.97       114\n",
            "weighted avg       0.97      0.97      0.97       114\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 8: Write a Python program to:\n",
        "‚óè Train an SVM Classifier on the Wine dataset using GridSearchCV to find the best\n",
        "C and gamma.\n",
        "‚óè Print the best hyperparameters and accuracy.\n"
      ],
      "metadata": {
        "id": "rgWA8Esh9eHf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# 1. Load the Wine dataset\n",
        "data = load_wine()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# 2. Split into training and testing sets (80-20 split)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# 3. Define the parameter grid for GridSearchCV\n",
        "param_grid = {\n",
        "    'C': [0.1, 1, 10, 100],\n",
        "    'gamma': [1, 0.1, 0.01, 0.001],\n",
        "    'kernel': ['rbf']\n",
        "}\n",
        "\n",
        "# 4. Set up the GridSearch with SVM\n",
        "grid = GridSearchCV(SVC(), param_grid, cv=5, verbose=1)\n",
        "grid.fit(X_train, y_train)\n",
        "\n",
        "# 5. Predict and evaluate\n",
        "y_pred = grid.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# 6. Print best parameters and accuracy\n",
        "print(\"Best Hyperparameters:\", grid.best_params_)\n",
        "print(\"Test Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j4X3BzgD9e9h",
        "outputId": "e325884c-bccd-4048-9949-dd93035f8d40"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n",
            "Best Hyperparameters: {'C': 100, 'gamma': 0.001, 'kernel': 'rbf'}\n",
            "Test Accuracy: 0.8333333333333334\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 9: Write a Python program to:\n",
        "‚óè Train a Na√Øve Bayes Classifier on a synthetic text dataset (e.g. using\n",
        "sklearn.datasets.fetch_20newsgroups).\n",
        "‚óè Print the model's ROC-AUC score for its predictions."
      ],
      "metadata": {
        "id": "fDNvI3BQ9ykq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.preprocessing import label_binarize\n",
        "\n",
        "# 1. Load a subset of the 20 Newsgroups dataset (binary classification for ROC-AUC)\n",
        "categories = ['sci.space', 'rec.sport.hockey']\n",
        "data = fetch_20newsgroups(subset='all', categories=categories, remove=('headers', 'footers', 'quotes'))\n",
        "\n",
        "# 2. Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(data.data, data.target, test_size=0.2, random_state=42)\n",
        "\n",
        "# 3. Convert text data to TF-IDF features\n",
        "vectorizer = TfidfVectorizer()\n",
        "X_train_vec = vectorizer.fit_transform(X_train)\n",
        "X_test_vec = vectorizer.transform(X_test)\n",
        "\n",
        "# 4. Train Multinomial Na√Øve Bayes\n",
        "model = MultinomialNB()\n",
        "model.fit(X_train_vec, y_train)\n",
        "\n",
        "# 5. Predict probabilities\n",
        "y_probs = model.predict_proba(X_test_vec)[:, 1]  # probability for class 1\n",
        "\n",
        "# 6. Calculate ROC-AUC score\n",
        "auc = roc_auc_score(y_test, y_probs)\n",
        "\n",
        "# 7. Print result\n",
        "print(\"ROC-AUC Score:\", auc)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "62Zxu7Cc9zjz",
        "outputId": "2a369804-19a9-4e00-ffcf-bd820f6116f2"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROC-AUC Score: 0.9924732269145282\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Yw_aDHiP-Cu1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}